{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Real-Time Prediction\n",
        "\n",
        "**Objective**: Use our saved batch model to score a live simulated stream. This provides the \"wow\" moment with zero setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import mlflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 4.1: The \"Live Order\" Simulator\n",
        "\n",
        "**Goal**: Use the rate source (a built-in simulator) to manufacture a new stream of orders.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the trained model from Part 3\n",
        "runs = mlflow.search_runs()\n",
        "latest_run = runs.iloc[0]\n",
        "run_id = latest_run['run_id']\n",
        "\n",
        "model_uri = f\"runs:/{run_id}/my_tpch_order_value_model\"\n",
        "loaded_model = mlflow.spark.load_model(model_uri)\n",
        "\n",
        "print(f\"âœ“ Model loaded!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Simulated Stream\n",
        "\n",
        "The `rate` source generates a stream of timestamps at a specified rate - perfect for simulating live data!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create rate stream (generates timestamps)\n",
        "rate_stream = spark.readStream \\\n",
        "    .format(\"rate\") \\\n",
        "    .option(\"rowsPerSecond\", 1) \\\n",
        "    .load()\n",
        "\n",
        "rate_stream.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Transform rate stream into order data (simple simulation)\n",
        "orders_stream = rate_stream.select(\n",
        "    col(\"timestamp\").alias(\"order_time\"),\n",
        "    # Simulate the 3 features our model needs\n",
        "    (rand() * 12 + 1).cast(\"int\").alias(\"month\"),\n",
        "    (rand() * 100000 - 1000).cast(\"double\").alias(\"c_acctbal\"),\n",
        "    # Market segment\n",
        "    when(rand() > 0.8, \"AUTOMOBILE\")\n",
        "    .when(rand() > 0.6, \"BUILDING\")\n",
        "    .when(rand() > 0.4, \"MACHINERY\")\n",
        "    .when(rand() > 0.2, \"HOUSEHOLD\")\n",
        "    .otherwise(\"FURNITURE\").alias(\"c_mktsegment\")\n",
        ")\n",
        "\n",
        "orders_stream.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 4.2: Apply Model & Display Live\n",
        "\n",
        "**Goal**: See live predictions in the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Features for the Model\n",
        "\n",
        "We need to apply the same feature engineering pipeline that was used during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# The loaded model is a full pipeline that includes feature engineering!\n",
        "# It will automatically apply StringIndexer and VectorAssembler\n",
        "# We just need to provide the raw features matching the pipeline input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Model to Stream\n",
        "\n",
        "The model can be applied directly to streaming DataFrames!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply model to stream (pipeline handles feature engineering automatically!)\n",
        "predictions_stream = loaded_model.transform(orders_stream)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display Live Predictions\n",
        "\n",
        "Use `display()` (Databricks) or write to console/sink for live predictions in Jupyter notebooks!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display live predictions\n",
        "# In Databricks: use display() for live updates\n",
        "# In local: use console sink\n",
        "query = predictions_stream.select(\n",
        "    \"order_time\",\n",
        "    \"c_mktsegment\",\n",
        "    \"c_acctbal\",\n",
        "    \"prediction\"\n",
        ").writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# In Databricks, use: display(predictions_stream.select(\"order_time\", \"c_mktsegment\", \"c_acctbal\", \"prediction\"))\n",
        "\n",
        "print(\"Streaming started! Check console for predictions.\")\n",
        "print(\"To stop: query.stop()\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Live Predictions!\n",
        "\n",
        "**What you're seeing**:\n",
        "- A stream generating predictions every second\n",
        "- Each row is a new order being scored in real-time\n",
        "- The `prediction` column shows predicted order value\n",
        "\n",
        "**In production**: Real data from Kafka/Kinesis, running 24/7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **Streaming**: Use `rate` for simulation, Kafka/Kinesis for production\n",
        "2. **Models**: ML models work seamlessly with streaming DataFrames\n",
        "3. **Real-Time**: Same batch model scores live data\n",
        "4. **Display**: Use `display()` in Databricks for live updates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Production Considerations\n",
        "\n",
        "**For real production streaming**:\n",
        "- Use Kafka, Kinesis, or Event Hub as source\n",
        "- Write predictions to Delta Lake or database\n",
        "- Enable checkpointing for fault tolerance\n",
        "- Monitor with Spark UI and alerting\n",
        "\n",
        "**Example production code**:\n",
        "```python\n",
        "query = predictions_stream.writeStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start(\"dbfs:/predictions/\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ‰ Congratulations! Workshop Complete!\n",
        "\n",
        "### What You've Built Today\n",
        "\n",
        "In just 2 hours, you've created a **production-grade, end-to-end data pipeline**:\n",
        "\n",
        "1. âœ… **PySpark Fundamentals**\n",
        "   - Mastered lazy evaluation (transformations vs actions)\n",
        "   - Learned DataFrame API (select, filter, withColumn, groupBy)\n",
        "   - Used Spark UI for debugging\n",
        "\n",
        "2. âœ… **Data Engineering Skills**\n",
        "   - Performed complex joins and aggregations\n",
        "   - Cleaned and transformed large datasets\n",
        "   - Saved data to Delta Lake (ACID transactions + time travel)\n",
        "\n",
        "3. âœ… **Machine Learning Pipeline**\n",
        "   - Built feature engineering pipeline (StringIndexer, VectorAssembler)\n",
        "   - Trained RandomForestRegressor for predictive analytics\n",
        "   - Tracked experiments with MLflow (automatic versioning)\n",
        "\n",
        "4. âœ… **Real-Time Streaming**\n",
        "   - Created simulated streaming data source\n",
        "   - Applied batch ML model to live stream\n",
        "   - Implemented real-time prediction system\n",
        "\n",
        "### ðŸ’¼ Resume-Worthy Achievement\n",
        "\n",
        "You can now confidently add to your resume:\n",
        "\n",
        "> **\"Built an end-to-end ML pipeline with PySpark and MLlib on Databricks, including feature engineering, model training with MLflow experiment tracking, and real-time prediction using Structured Streaming. Processed large-scale datasets with distributed computing and deployed production-ready data pipelines using Delta Lake.\"**\n",
        "\n",
        "This is **real, industry-relevant experience** that employers value!\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Next Steps: Your Learning Journey\n",
        "\n",
        "### Week 1: Solidify Your Skills\n",
        "- [ ] **Re-run the workshop** on your own (muscle memory!)\n",
        "- [ ] **Experiment**: Change model parameters, add more features\n",
        "- [ ] **Try different datasets**: Explore `/databricks-datasets/`\n",
        "- [ ] **Modify the pipeline**: Use different ML algorithm (GBT, Logistic Regression)\n",
        "\n",
        "### Month 1: Expand Your Knowledge\n",
        "\n",
        "#### Free Learning Resources\n",
        "1. **Databricks Academy** (100% free courses):\n",
        "   - \"Apache Spark Programming with Databricks\" (4 hours)\n",
        "   - \"Data Engineering with Databricks\" (6 hours)\n",
        "   - \"Machine Learning on Databricks\" (5 hours)\n",
        "   - Link: https://academy.databricks.com/\n",
        "\n",
        "2. **Practice Projects**:\n",
        "   - Customer segmentation with K-Means clustering\n",
        "   - Churn prediction with Gradient Boosted Trees\n",
        "   - Time series forecasting (IoT sensor data)\n",
        "   - Recommendation system with ALS (MovieLens dataset)\n",
        "\n",
        "3. **Read Documentation**:\n",
        "   - PySpark API: https://spark.apache.org/docs/latest/api/python/\n",
        "   - MLlib Guide: https://spark.apache.org/docs/latest/ml-guide.html\n",
        "   - Delta Lake: https://docs.delta.io/\n",
        "\n",
        "#### Suggested Mini-Projects\n",
        "```python\n",
        "# Project 1: Customer Segmentation (2-3 hours)\n",
        "# Dataset: /databricks-datasets/retail-org/customers/\n",
        "# Goal: Use K-Means to segment customers by behavior\n",
        "# Skills: Feature engineering, clustering, visualization\n",
        "\n",
        "# Project 2: Flight Delay Prediction (3-4 hours)\n",
        "# Dataset: /databricks-datasets/airlines/\n",
        "# Goal: Predict flight delays using classification\n",
        "# Skills: Multi-class classification, cross-validation, hyperparameter tuning\n",
        "\n",
        "# Project 3: IoT Anomaly Detection (4-5 hours)\n",
        "# Dataset: /databricks-datasets/iot/\n",
        "# Goal: Detect anomalous sensor readings in real-time\n",
        "# Skills: Streaming, anomaly detection, windowing\n",
        "```\n",
        "\n",
        "### Month 2-3: Build Portfolio & Get Certified\n",
        "\n",
        "#### Build Portfolio Projects\n",
        "1. **Create a public GitHub repo**:\n",
        "   - Well-documented README with business context\n",
        "   - Jupyter notebooks or scripts\n",
        "   - Results visualization (charts, dashboards)\n",
        "   - Clear instructions to reproduce\n",
        "\n",
        "2. **Project Ideas**:\n",
        "   - NYC Taxi: Trip duration prediction + surge pricing analysis\n",
        "   - E-commerce: Market basket analysis + product recommendation\n",
        "   - Social media: Sentiment analysis + trend detection\n",
        "   - Healthcare: Patient readmission risk prediction\n",
        "\n",
        "3. **Share Your Work**:\n",
        "   - LinkedIn posts with visualizations\n",
        "   - Medium/Dev.to blog posts\n",
        "   - Present at local meetups\n",
        "   - Contribute to open-source Spark projects\n",
        "\n",
        "#### Pursue Certification\n",
        "- **Databricks Certified Data Engineer Associate**\n",
        "  - Cost: $200\n",
        "  - Study guide: https://databricks.com/learn/certification\n",
        "  - Practice exams available\n",
        "  \n",
        "- **Databricks Certified ML Associate**\n",
        "  - Focuses on MLlib and MLflow\n",
        "  - Great for ML engineers and data scientists\n",
        "\n",
        "### Advanced Topics to Explore\n",
        "\n",
        "#### Performance Optimization\n",
        "```python\n",
        "# Broadcast joins for small tables\n",
        "from pyspark.sql.functions import broadcast\n",
        "large_df.join(broadcast(small_df), \"id\")\n",
        "\n",
        "# Repartitioning for better parallelism\n",
        "df.repartition(100, \"customer_id\")\n",
        "\n",
        "# Z-ordering for Delta Lake\n",
        "spark.sql(\"OPTIMIZE delta.`/path/` ZORDER BY (customer_id, date)\")\n",
        "\n",
        "# Adaptive Query Execution (AQE)\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "#### Advanced Streaming\n",
        "```python\n",
        "# Stateful streaming with watermarking\n",
        "streaming_df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"user_id\") \\\n",
        "    .count()\n",
        "\n",
        "# Stream-to-stream joins\n",
        "stream1.join(stream2, \"user_id\", \"inner\")\n",
        "\n",
        "# Kafka integration (production)\n",
        "df = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"host:port\") \\\n",
        "    .option(\"subscribe\", \"topic\") \\\n",
        "    .load()\n",
        "```\n",
        "\n",
        "#### Delta Lake Advanced Features\n",
        "```python\n",
        "# Time travel (query old versions)\n",
        "df = spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", 5) \\\n",
        "    .load(\"/path/to/delta\")\n",
        "\n",
        "# Merge (upsert) operations\n",
        "from delta.tables import DeltaTable\n",
        "delta_table = DeltaTable.forPath(spark, \"/path/\")\n",
        "delta_table.merge(updates, \"target.id = source.id\") \\\n",
        "    .whenMatchedUpdate(set = {\"value\": \"source.value\"}) \\\n",
        "    .whenNotMatchedInsert(values = {\"id\": \"source.id\", \"value\": \"source.value\"}) \\\n",
        "    .execute()\n",
        "\n",
        "# Schema evolution\n",
        "df.write.format(\"delta\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .save(\"/path/\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ Career Pathways\n",
        "\n",
        "### Roles You're Now Prepared For\n",
        "\n",
        "1. **Data Engineer** (Entry-Level)\n",
        "   - Build and maintain data pipelines\n",
        "   - ETL/ELT development\n",
        "   - Data quality monitoring\n",
        "   - Average salary: $85k-$110k\n",
        "\n",
        "2. **Analytics Engineer**\n",
        "   - Transform raw data for BI/analytics\n",
        "   - Write SQL and PySpark for transformations\n",
        "   - Collaborate with analysts and data scientists\n",
        "   - Average salary: $80k-$105k\n",
        "\n",
        "3. **ML Engineer** (Junior)\n",
        "   - Deploy ML models to production\n",
        "   - Build feature pipelines\n",
        "   - Monitor model performance\n",
        "   - Average salary: $90k-$120k\n",
        "\n",
        "4. **Data Scientist** (with engineering skills)\n",
        "   - Analyze data at scale with PySpark\n",
        "   - Build and deploy ML models\n",
        "   - Communicate insights to stakeholders\n",
        "   - Average salary: $95k-$130k\n",
        "\n",
        "### Skills to Add Next\n",
        "\n",
        "**For Data Engineers:**\n",
        "- Airflow/Databricks Workflows (orchestration)\n",
        "- dbt (transformation framework)\n",
        "- CI/CD for data pipelines\n",
        "- Data quality frameworks (Great Expectations)\n",
        "- Cloud platforms (AWS/Azure/GCP)\n",
        "\n",
        "**For ML Engineers:**\n",
        "- Model serving (MLflow, FastAPI, Kubernetes)\n",
        "- A/B testing frameworks\n",
        "- Model monitoring (drift detection)\n",
        "- Feature stores (Databricks Feature Store, Feast)\n",
        "- Advanced MLOps practices\n",
        "\n",
        "**For Data Scientists:**\n",
        "- Deep learning (TensorFlow, PyTorch)\n",
        "- Advanced algorithms (XGBoost, LightGBM)\n",
        "- Statistical analysis (hypothesis testing, causal inference)\n",
        "- Experimentation platforms\n",
        "- Data visualization (Tableau, Power BI)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŒŸ Community & Support\n",
        "\n",
        "### Join the Community\n",
        "\n",
        "**Databricks Community:**\n",
        "- Forums: https://community.databricks.com/\n",
        "- LinkedIn: Follow #Databricks, #ApacheSpark\n",
        "- Twitter: @databricks, @apachespark\n",
        "- Slack: Various Spark communities\n",
        "\n",
        "**Open Source:**\n",
        "- Apache Spark: https://spark.apache.org/community.html\n",
        "- Contribute to Spark projects on GitHub\n",
        "- Answer questions on Stack Overflow (tag: `pyspark`, `databricks`)\n",
        "\n",
        "**Local Meetups:**\n",
        "- Search Meetup.com for \"Spark\" or \"Big Data\"\n",
        "- Attend virtual conferences (Spark + AI Summit)\n",
        "- Join data engineering Discord servers\n",
        "\n",
        "### Stay Updated\n",
        "\n",
        "**Blogs to Follow:**\n",
        "- Databricks Blog: https://databricks.com/blog\n",
        "- Apache Spark Blog: https://spark.apache.org/news/\n",
        "- Towards Data Science (Medium)\n",
        "- KDnuggets\n",
        "\n",
        "**Podcasts:**\n",
        "- Data Engineering Podcast\n",
        "- The Data Stack Show\n",
        "- Software Engineering Daily (data episodes)\n",
        "\n",
        "**YouTube Channels:**\n",
        "- Databricks\n",
        "- Advancing Analytics\n",
        "- Data Council\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Workshop Feedback\n",
        "\n",
        "We'd love to hear from you! Your feedback helps us improve.\n",
        "\n",
        "**What worked well?**\n",
        "- What concepts clicked?\n",
        "- What exercises were most valuable?\n",
        "- What should we keep?\n",
        "\n",
        "**What could be better?**\n",
        "- What was confusing?\n",
        "- What topics need more depth?\n",
        "- What would you like to see added?\n",
        "\n",
        "**Share your feedback:**\n",
        "- GitHub issues: [Link to repo]\n",
        "- Email: [instructor email]\n",
        "- Survey: [Link to survey]\n",
        "\n",
        "**Share your success:**\n",
        "- Built a project? Share it on LinkedIn! Tag us\n",
        "- Got certified? Celebrate with the community\n",
        "- Found a job? Let us know - we'd love to hear!\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ† Final Thoughts\n",
        "\n",
        "### You've Accomplished Something Real\n",
        "\n",
        "Many tutorials teach theory. You built a **real system**:\n",
        "- Processed large-scale data efficiently\n",
        "- Trained a production-quality ML model\n",
        "- Deployed real-time predictions\n",
        "- Used industry-standard tools (Databricks, MLflow, Delta)\n",
        "\n",
        "This is **the same stack** used by:\n",
        "- Netflix (recommendation systems)\n",
        "- Uber (surge pricing, ETAs)\n",
        "- Airbnb (pricing optimization)\n",
        "- Microsoft (Azure analytics)\n",
        "- Thousands of other companies\n",
        "\n",
        "### The Journey Continues\n",
        "\n",
        "Becoming proficient with Spark is a journey. You've taken the first big step today:\n",
        "- âœ… You understand the fundamentals\n",
        "- âœ… You've written real code\n",
        "- âœ… You've built an end-to-end pipeline\n",
        "- âœ… You have a foundation to build on\n",
        "\n",
        "**Next step: Build something.** The best learning happens when you're solving real problems.\n",
        "\n",
        "### Remember\n",
        "\n",
        "> **\"The expert in anything was once a beginner.\"**\n",
        "\n",
        "You started as a beginner this morning. You're already further than 95% of people who say \"I should learn Spark someday.\"\n",
        "\n",
        "**Keep going. Keep building. Keep learning.**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ Additional Resources Checklist\n",
        "\n",
        "Save these links for future reference:\n",
        "\n",
        "### Documentation\n",
        "- [ ] PySpark API: https://spark.apache.org/docs/latest/api/python/\n",
        "- [ ] Databricks Docs: https://docs.databricks.com/\n",
        "- [ ] Delta Lake: https://docs.delta.io/\n",
        "- [ ] MLflow: https://mlflow.org/docs/latest/index.html\n",
        "\n",
        "### Learning Platforms\n",
        "- [ ] Databricks Academy: https://academy.databricks.com/\n",
        "- [ ] Coursera: \"Big Data Analysis with Scala and Spark\"\n",
        "- [ ] Udemy: Search \"PySpark\" (check ratings!)\n",
        "- [ ] YouTube: Databricks channel\n",
        "\n",
        "### Practice\n",
        "- [ ] Kaggle competitions (use PySpark for large datasets)\n",
        "- [ ] Databricks sample notebooks: https://docs.databricks.com/notebooks/gallery.html\n",
        "- [ ] GitHub: Search \"pyspark projects\" for inspiration\n",
        "\n",
        "### Reference Materials\n",
        "- [ ] This workshop repo (star it!)\n",
        "- [ ] PySpark Cheat Sheet: [PYSPARK_CHEATSHEET.md](PYSPARK_CHEATSHEET.md)\n",
        "- [ ] Workshop Guide: [WORKSHOP_GUIDE.md](WORKSHOP_GUIDE.md)\n",
        "- [ ] Databricks Setup: [DATABRICKS_FREE_EDITION_GUIDE.md](DATABRICKS_FREE_EDITION_GUIDE.md)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Your Action Plan (Next 7 Days)\n",
        "\n",
        "### Day 1 (Today): Solidify\n",
        "- [ ] Re-run all 4 notebooks on your own\n",
        "- [ ] Experiment: Change one parameter in the ML model\n",
        "- [ ] Save notebooks to your local machine (backup)\n",
        "\n",
        "### Day 2-3: Explore\n",
        "- [ ] Load a different Databricks dataset\n",
        "- [ ] Perform basic analysis (groupBy, aggregations)\n",
        "- [ ] Create simple visualizations\n",
        "\n",
        "### Day 4-5: Build\n",
        "- [ ] Start a mini-project (2-3 hours)\n",
        "- [ ] Use a different ML algorithm (try GBTRegressor)\n",
        "- [ ] Save results to your GitHub\n",
        "\n",
        "### Day 6-7: Share\n",
        "- [ ] Write a LinkedIn post about what you learned\n",
        "- [ ] Share your mini-project\n",
        "- [ ] Connect with 3 people in the data community\n",
        "\n",
        "**Set a calendar reminder now!** Don't let momentum fade.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ‰ Congratulations Again!\n",
        "\n",
        "You did it! You spent 2 hours investing in yourself and learning valuable, in-demand skills.\n",
        "\n",
        "**From all of us: Thank you for your time, attention, and curiosity.**\n",
        "\n",
        "**Now go build something amazing with PySpark! âš¡**\n",
        "\n",
        "---\n",
        "\n",
        "**Questions? Reach out anytime:**\n",
        "- GitHub Issues: [repo link]\n",
        "- Community Forums: https://community.databricks.com/\n",
        "- LinkedIn: [instructor profile]\n",
        "\n",
        "**Happy Sparking! ðŸš€âœ¨**\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stop the streaming query when done\n",
        "# Uncomment the line below to stop the stream\n",
        "query.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}