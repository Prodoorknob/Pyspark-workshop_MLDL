{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics: Job Orchestration & Additional Pitfalls\n",
    "## Companion Notebook to Performance Pitfalls Workshop\n",
    "\n",
    "This notebook covers:\n",
    "- Delta Lake transaction issues and best practices\n",
    "- Object store semantics (S3/Azure/GCS)\n",
    "- File formats and compression codecs comparison\n",
    "- Advanced job scheduling patterns\n",
    "- Alerting and retry strategies\n",
    "- Real-world orchestration examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# In Databricks, SparkSession is already available as 'spark'\n",
    "# Delta Lake is pre-configured, no need to set configurations\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Import Delta Table (available in Databricks by default)\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    print(\"‚úÖ Delta Lake is available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Delta Lake not available - some examples will be skipped\")\n",
    "    print(\"   (This is OK for learning other concepts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Delta Lake: Concurrency & Transaction Issues\n",
    "\n",
    "**What is it?**  \n",
    "Delta Lake provides ACID transactions, but improper usage leads to conflicts and performance problems.\n",
    "\n",
    "**Common Issues:**\n",
    "- Concurrent write conflicts\n",
    "- Small files problem\n",
    "- Vacuum timing issues\n",
    "- Transaction log growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Delta table for demonstration\n",
    "delta_path = \"/tmp/delta_sales\"\n",
    "\n",
    "# Initial data\n",
    "sales_data = [\n",
    "    (1, 'Product_A', 100, '2025-01-01', 'US'),\n",
    "    (2, 'Product_B', 200, '2025-01-01', 'UK'),\n",
    "    (3, 'Product_C', 150, '2025-01-02', 'US'),\n",
    "    (4, 'Product_A', 300, '2025-01-02', 'UK'),\n",
    "    (5, 'Product_B', 250, '2025-01-03', 'US'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sales_data, ['id', 'product', 'amount', 'date', 'region'])\n",
    "\n",
    "# Write as Delta table\n",
    "df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "\n",
    "print(\"‚úÖ Delta table created\")\n",
    "spark.read.format('delta').load(delta_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1: Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Multiple small writes create too many small files\n",
    "print(\"BAD: Creating many small files...\")\n",
    "\n",
    "# Simulate streaming writes (creates many small files)\n",
    "for i in range(10):\n",
    "    small_batch = spark.createDataFrame(\n",
    "        [(i + 100, f'Product_{i}', i * 10, '2025-01-04', 'US')],\n",
    "        ['id', 'product', 'amount', 'date', 'region']\n",
    "    )\n",
    "    small_batch.write.format('delta').mode('append').save(delta_path)\n",
    "\n",
    "# Check the number of files\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "file_count = len(delta_table.toDF().inputFiles())\n",
    "print(f\"\\n‚ö†Ô∏è  Total files: {file_count}\")\n",
    "print(\"Problem: Too many small files slow down reads!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Optimize to compact small files\n",
    "print(\"GOOD: Compacting small files with OPTIMIZE...\")\n",
    "\n",
    "# Run OPTIMIZE to compact files\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "# Check file count after optimization\n",
    "delta_table_optimized = DeltaTable.forPath(spark, delta_path)\n",
    "optimized_file_count = len(delta_table_optimized.toDF().inputFiles())\n",
    "print(f\"\\n‚úÖ Files after OPTIMIZE: {optimized_file_count}\")\n",
    "print(f\"Reduced files by: {file_count - optimized_file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ BETTER: Auto-optimize for streaming workloads\n",
    "print(\"BETTER: Enable auto-optimize for future writes\")\n",
    "\n",
    "# Enable auto-optimize and auto-compaction\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE delta.`{delta_path}` \n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Auto-optimize enabled!\")\n",
    "print(\"Future writes will automatically compact files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2: Concurrent Write Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate merge/upsert pattern (safe for concurrent operations)\n",
    "print(\"SAFE PATTERN: Using MERGE for upserts (avoids conflicts)\")\n",
    "\n",
    "# New data to upsert\n",
    "updates = spark.createDataFrame([\n",
    "    (1, 'Product_A', 150, '2025-01-05', 'US'),  # Update existing\n",
    "    (200, 'Product_Z', 500, '2025-01-05', 'JP'), # Insert new\n",
    "], ['id', 'product', 'amount', 'date', 'region'])\n",
    "\n",
    "# Perform merge (upsert)\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "delta_table.alias('target').merge(\n",
    "    updates.alias('source'),\n",
    "    'target.id = source.id'\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"\\n‚úÖ Merge completed successfully\")\n",
    "spark.read.format('delta').load(delta_path).filter(col('id').isin(1, 200)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #3: Vacuum Timing and Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding VACUUM and time travel\n",
    "print(\"Understanding VACUUM and retention...\")\n",
    "\n",
    "# Check history\n",
    "print(\"\\nTable history:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\").select(\n",
    "    'version', 'timestamp', 'operation', 'operationMetrics'\n",
    ").show(truncate=False)\n",
    "\n",
    "# Time travel example\n",
    "print(\"\\nTime travel - reading version 0:\")\n",
    "spark.read.format('delta').option('versionAsOf', 0).load(delta_path).show()\n",
    "\n",
    "print(\"\"\"\n",
    "‚ö†Ô∏è  VACUUM Best Practices:\n",
    "\n",
    "1. Default retention: 7 days\n",
    "2. Don't VACUUM too soon (breaks time travel!)\n",
    "3. Set appropriate retention for your use case:\n",
    "   - Development: 7 days (default)\n",
    "   - Production: 30+ days\n",
    "   - Compliance: 90+ days\n",
    "\n",
    "4. VACUUM removes old files to save storage\n",
    "5. But you can't time travel past VACUUM point!\n",
    "\"\"\")\n",
    "\n",
    "# Set retention period\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE delta.`{delta_path}`\n",
    "    SET TBLPROPERTIES ('delta.deletedFileRetentionDuration' = 'interval 30 days')\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Retention period set to 30 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Delta Lake Key Takeaways:**\n",
    "- Run `OPTIMIZE` regularly to compact small files\n",
    "- Enable auto-optimize for streaming workloads\n",
    "- Use `MERGE` for upserts (handles concurrency)\n",
    "- Set appropriate retention periods\n",
    "- Don't `VACUUM` too aggressively\n",
    "- Use `Z-ORDER` for frequently filtered columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Object Store Semantics (S3/Azure/GCS)\n",
    "\n",
    "**What is it?**  \n",
    "Cloud object stores are not filesystems - they have different consistency and performance characteristics.\n",
    "\n",
    "**Key Issues:**\n",
    "- LIST operations are slow and expensive\n",
    "- Eventual consistency (S3)\n",
    "- Rate limiting\n",
    "- Small file performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìä OBJECT STORE BEST PRACTICES:\n",
    "\n",
    "1. S3 Specific:\n",
    "   ‚úÖ Use S3A filesystem (s3a://)\n",
    "   ‚úÖ Enable S3 Select for filtered reads\n",
    "   ‚úÖ Use S3 request rate limits in configs\n",
    "   ‚úÖ Bucket naming: avoid sequential prefixes\n",
    "   \n",
    "2. Azure Blob/ADLS:\n",
    "   ‚úÖ Use ADLS Gen2 (better performance)\n",
    "   ‚úÖ Enable hierarchical namespace\n",
    "   ‚úÖ Use appropriate access tiers\n",
    "   \n",
    "3. GCS (Google Cloud Storage):\n",
    "   ‚úÖ Use composite objects for large files\n",
    "   ‚úÖ Enable parallel composite uploads\n",
    "   \n",
    "4. General:\n",
    "   ‚úÖ Minimize LIST operations (use partitioning)\n",
    "   ‚úÖ Write larger files (128MB+)\n",
    "   ‚úÖ Use columnar formats (Parquet/ORC)\n",
    "   ‚úÖ Enable cloud-specific optimizations\n",
    "   ‚ùå Don't treat like local filesystem!\n",
    "   ‚ùå Avoid RENAME operations (copy + delete)\n",
    "   ‚ùå Don't have too many small files\n",
    "\"\"\")\n",
    "\n",
    "# Example configurations for S3\n",
    "s3_configs = {\n",
    "    # Connection pooling\n",
    "    \"fs.s3a.connection.maximum\": \"100\",\n",
    "    \n",
    "    # Enable multipart uploads\n",
    "    \"fs.s3a.multipart.size\": \"104857600\",  # 100MB\n",
    "    \"fs.s3a.multipart.threshold\": \"209715200\",  # 200MB\n",
    "    \n",
    "    # Fast upload\n",
    "    \"fs.s3a.fast.upload\": \"true\",\n",
    "    \"fs.s3a.fast.upload.buffer\": \"disk\",\n",
    "    \n",
    "    # Performance tuning\n",
    "    \"fs.s3a.threads.max\": \"50\",\n",
    "    \"fs.s3a.connection.ssl.enabled\": \"true\",\n",
    "}\n",
    "\n",
    "print(\"\\nExample S3 Configurations:\")\n",
    "for key, value in s3_configs.items():\n",
    "    print(f\"  {key} = {value}\")\n",
    "    # spark.conf.set(key, value)  # Uncomment when using S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## File Formats & Compression Codecs\n",
    "\n",
    "**What is it?**  \n",
    "Choice of file format and compression codec significantly impacts performance and storage costs.\n",
    "\n",
    "**Key Considerations:**\n",
    "- Read vs write performance\n",
    "- Compression ratio vs CPU cost\n",
    "- Splittability for parallelism\n",
    "- Schema evolution support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_data = spark.range(0, 1000000).select(\n",
    "    col('id'),\n",
    "    (col('id') % 100).alias('category'),\n",
    "    (rand() * 1000).alias('value'),\n",
    "    concat(lit('text_'), col('id').cast('string')).alias('description')\n",
    ")\n",
    "\n",
    "test_data.cache().count()\n",
    "print(f\"Test data created: {test_data.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Compare different formats\n",
    "formats = ['parquet', 'orc', 'csv', 'json']\n",
    "results = []\n",
    "\n",
    "for fmt in formats:\n",
    "    path = f\"/tmp/format_test_{fmt}\"\n",
    "    \n",
    "    # Write\n",
    "    start = time.time()\n",
    "    test_data.write.format(fmt).mode('overwrite').save(path)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # Get size (approximate for demo)\n",
    "    size = \"N/A\"  # In production, calculate actual size\n",
    "    \n",
    "    # Read\n",
    "    start = time.time()\n",
    "    read_df = spark.read.format(fmt).load(path)\n",
    "    read_df.count()\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    results.append((fmt, write_time, read_time, size))\n",
    "\n",
    "# Display results\n",
    "results_df = spark.createDataFrame(results, ['format', 'write_time_sec', 'read_time_sec', 'size'])\n",
    "print(\"\\nFormat Performance Comparison:\")\n",
    "results_df.show()\n",
    "\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare compression codecs for Parquet\n",
    "codecs = ['snappy', 'gzip', 'lz4', 'uncompressed']\n",
    "codec_results = []\n",
    "\n",
    "test_data_small = spark.range(100000).select(\n",
    "    col('id'),\n",
    "    concat(lit('text_'), col('id').cast('string')).alias('text')\n",
    ").cache()\n",
    "test_data_small.count()\n",
    "\n",
    "for codec in codecs:\n",
    "    path = f\"/tmp/codec_test_{codec}\"\n",
    "    \n",
    "    try:\n",
    "        # Write with compression\n",
    "        start = time.time()\n",
    "        test_data_small.write \\\n",
    "            .format('parquet') \\\n",
    "            .option('compression', codec) \\\n",
    "            .mode('overwrite') \\\n",
    "            .save(path)\n",
    "        write_time = time.time() - start\n",
    "        \n",
    "        # Read\n",
    "        start = time.time()\n",
    "        spark.read.parquet(path).count()\n",
    "        read_time = time.time() - start\n",
    "        \n",
    "        codec_results.append((codec, write_time, read_time))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {codec} not available: {e}\")\n",
    "\n",
    "# Display results\n",
    "if codec_results:\n",
    "    codec_df = spark.createDataFrame(codec_results, ['codec', 'write_time_sec', 'read_time_sec'])\n",
    "    print(\"\\nCompression Codec Comparison (Parquet):\")\n",
    "    codec_df.show()\n",
    "\n",
    "test_data_small.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìä FILE FORMAT RECOMMENDATIONS:\n",
    "\n",
    "üèÜ PARQUET (Best for most use cases):\n",
    "   ‚úÖ Columnar format (great for analytics)\n",
    "   ‚úÖ Excellent compression\n",
    "   ‚úÖ Predicate pushdown\n",
    "   ‚úÖ Schema evolution support\n",
    "   ‚úÖ Industry standard\n",
    "   üéØ Use with: Snappy compression (balanced)\n",
    "   \n",
    "ü•à ORC (Alternative to Parquet):\n",
    "   ‚úÖ Slightly better compression than Parquet\n",
    "   ‚úÖ Built-in indexes\n",
    "   ‚úÖ Native to Hive ecosystem\n",
    "   ‚ö†Ô∏è  Less widespread adoption than Parquet\n",
    "   \n",
    "üìÑ DELTA (Parquet + ACID transactions):\n",
    "   ‚úÖ All Parquet benefits + transactions\n",
    "   ‚úÖ Time travel\n",
    "   ‚úÖ Schema evolution\n",
    "   ‚úÖ MERGE/UPDATE/DELETE support\n",
    "   üéØ Recommended for production data lakes\n",
    "   \n",
    "‚ùå CSV/JSON (Avoid for large data):\n",
    "   ‚ùå No compression (or inefficient)\n",
    "   ‚ùå No schema enforcement\n",
    "   ‚ùå No predicate pushdown\n",
    "   ‚ùå Slow to parse\n",
    "   ‚úÖ Only use for: data exchange, small files\n",
    "\n",
    "üîê COMPRESSION CODECS:\n",
    "\n",
    "   SNAPPY (default, recommended):\n",
    "     ‚úÖ Fast compression/decompression\n",
    "     ‚úÖ Good compression ratio\n",
    "     ‚úÖ Splittable\n",
    "     üéØ Best for: General use\n",
    "     \n",
    "   GZIP:\n",
    "     ‚úÖ Better compression than Snappy\n",
    "     ‚ùå Slower decompression\n",
    "     ‚ùå Not splittable\n",
    "     üéØ Best for: Cold storage, rarely read data\n",
    "     \n",
    "   LZ4:\n",
    "     ‚úÖ Fastest decompression\n",
    "     ‚ö†Ô∏è  Lower compression ratio\n",
    "     üéØ Best for: Hot data, frequently queried\n",
    "     \n",
    "   ZSTD:\n",
    "     ‚úÖ Excellent compression ratio\n",
    "     ‚úÖ Fast decompression\n",
    "     üéØ Best for: Newer Spark versions (3.2+)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Job Scheduling Patterns\n",
    "\n",
    "Let's look at real-world job orchestration patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Incremental Processing with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental processing pattern\n",
    "class IncrementalProcessor:\n",
    "    \"\"\"\n",
    "    Process only new data since last run using checkpoints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "    \n",
    "    def get_last_checkpoint(self):\n",
    "        \"\"\"Read last processed timestamp\"\"\"\n",
    "        try:\n",
    "            checkpoint_df = spark.read.parquet(self.checkpoint_path)\n",
    "            last_timestamp = checkpoint_df.agg(max('processed_until')).collect()[0][0]\n",
    "            return last_timestamp\n",
    "        except:\n",
    "            # No checkpoint exists, process from beginning\n",
    "            return '2025-01-01 00:00:00'\n",
    "    \n",
    "    def save_checkpoint(self, timestamp):\n",
    "        \"\"\"Save current processing timestamp\"\"\"\n",
    "        checkpoint_data = spark.createDataFrame(\n",
    "            [(timestamp, datetime.now().isoformat())],\n",
    "            ['processed_until', 'checkpoint_time']\n",
    "        )\n",
    "        checkpoint_data.write.mode('append').parquet(self.checkpoint_path)\n",
    "    \n",
    "    def process_incremental(self, source_path, target_path):\n",
    "        \"\"\"Process only new data\"\"\"\n",
    "        print(\"üîÑ Starting incremental processing...\")\n",
    "        \n",
    "        # Get last checkpoint\n",
    "        last_processed = self.get_last_checkpoint()\n",
    "        print(f\"   Last processed: {last_processed}\")\n",
    "        \n",
    "        # Read only new data\n",
    "        source_df = spark.read.format('delta').load(source_path)\n",
    "        new_data = source_df.filter(col('date') > last_processed)\n",
    "        \n",
    "        new_count = new_data.count()\n",
    "        print(f\"   New records to process: {new_count:,}\")\n",
    "        \n",
    "        if new_count > 0:\n",
    "            # Process and write\n",
    "            processed = new_data.groupBy('region', 'product').agg(\n",
    "                sum('amount').alias('total_sales'),\n",
    "                count('*').alias('transaction_count')\n",
    "            )\n",
    "            \n",
    "            # Append to target\n",
    "            processed.write.format('delta').mode('append').save(target_path)\n",
    "            \n",
    "            # Update checkpoint\n",
    "            max_date = new_data.agg(max('date')).collect()[0][0]\n",
    "            self.save_checkpoint(max_date)\n",
    "            \n",
    "            print(f\"   ‚úÖ Processed {new_count:,} records\")\n",
    "            print(f\"   ‚úÖ Updated checkpoint to: {max_date}\")\n",
    "        else:\n",
    "            print(\"   ‚è≠Ô∏è  No new data to process\")\n",
    "        \n",
    "        return new_count\n",
    "\n",
    "# Demo\n",
    "processor = IncrementalProcessor('/tmp/checkpoint_demo')\n",
    "records_processed = processor.process_incremental(delta_path, '/tmp/aggregated_sales')\n",
    "\n",
    "print(\"\\nüí° This pattern ensures you only process new data each run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Retry Logic with Exponential Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def retry_with_backoff(func, max_retries=3, initial_delay=1, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Retry function with exponential backoff\n",
    "    \"\"\"\n",
    "    delay = initial_delay\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nüîÑ Attempt {attempt + 1}/{max_retries}...\")\n",
    "            result = func()\n",
    "            print(\"   ‚úÖ Success!\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                # Add jitter to prevent thundering herd\n",
    "                jitter = random.uniform(0, delay * 0.1)\n",
    "                sleep_time = delay + jitter\n",
    "                \n",
    "                print(f\"   ‚è≥ Waiting {sleep_time:.2f} seconds before retry...\")\n",
    "                time.sleep(sleep_time)\n",
    "                \n",
    "                # Exponential backoff\n",
    "                delay *= backoff_factor\n",
    "            else:\n",
    "                print(\"   üí• Max retries reached, giving up\")\n",
    "                raise\n",
    "\n",
    "# Demo: Simulate unreliable operation\n",
    "attempt_count = 0\n",
    "\n",
    "def unreliable_operation():\n",
    "    global attempt_count\n",
    "    attempt_count += 1\n",
    "    \n",
    "    # Fail first 2 times, succeed on 3rd\n",
    "    if attempt_count < 3:\n",
    "        raise Exception(f\"Simulated failure (attempt {attempt_count})\")\n",
    "    \n",
    "    return \"Success!\"\n",
    "\n",
    "# Run with retry logic\n",
    "print(\"Demonstrating retry with exponential backoff:\")\n",
    "result = retry_with_backoff(unreliable_operation, max_retries=5)\n",
    "print(f\"\\nFinal result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Circuit Breaker for Downstream Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern to prevent cascading failures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold=3, timeout=60):\n",
    "        self.failure_count = 0\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.last_failure_time = None\n",
    "        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n",
    "    \n",
    "    def call(self, func):\n",
    "        \"\"\"\n",
    "        Execute function with circuit breaker protection\n",
    "        \"\"\"\n",
    "        # Check if circuit is open\n",
    "        if self.state == 'OPEN':\n",
    "            # Check if timeout has passed\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = 'HALF_OPEN'\n",
    "                print(\"üü° Circuit breaker HALF_OPEN (testing recovery)\")\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN - too many failures\")\n",
    "        \n",
    "        try:\n",
    "            result = func()\n",
    "            \n",
    "            # Success - reset failure count\n",
    "            if self.state == 'HALF_OPEN':\n",
    "                print(\"üü¢ Circuit breaker CLOSED (recovered)\")\n",
    "            \n",
    "            self.failure_count = 0\n",
    "            self.state = 'CLOSED'\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            print(f\"‚ùå Failure {self.failure_count}/{self.failure_threshold}\")\n",
    "            \n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                self.state = 'OPEN'\n",
    "                print(\"üî¥ Circuit breaker OPEN (too many failures)\")\n",
    "            \n",
    "            raise\n",
    "\n",
    "# Demo\n",
    "breaker = CircuitBreaker(failure_threshold=3, timeout=5)\n",
    "\n",
    "def flaky_api_call():\n",
    "    if random.random() < 0.7:  # 70% failure rate\n",
    "        raise Exception(\"API call failed\")\n",
    "    return \"API response\"\n",
    "\n",
    "print(\"Demonstrating circuit breaker:\")\n",
    "for i in range(6):\n",
    "    print(f\"\\n--- Call {i+1} ---\")\n",
    "    try:\n",
    "        result = breaker.call(flaky_api_call)\n",
    "        print(f\"‚úÖ Success: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\nüí° Circuit breaker prevents cascading failures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: Dead Letter Queue for Failed Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_dlq(input_df, output_path, dlq_path):\n",
    "    \"\"\"\n",
    "    Process data and send failures to Dead Letter Queue\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType, StructType, StructField\n",
    "    \n",
    "    print(\"üîÑ Processing with Dead Letter Queue...\")\n",
    "    \n",
    "    # Add processing status columns\n",
    "    def safe_process(value):\n",
    "        \"\"\"Process with error handling\"\"\"\n",
    "        try:\n",
    "            # Simulate processing logic\n",
    "            if value < 0:\n",
    "                raise ValueError(\"Negative values not allowed\")\n",
    "            return (value * 2, None)  # (result, error)\n",
    "        except Exception as e:\n",
    "            return (None, str(e))  # (result, error)\n",
    "    \n",
    "    # Use native Spark instead of UDF for better performance\n",
    "    processed_df = input_df.withColumn(\n",
    "        'processed_amount',\n",
    "        when(col('amount') >= 0, col('amount') * 2).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'error_message',\n",
    "        when(col('amount') < 0, 'Negative values not allowed').otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Split successful and failed records\n",
    "    success_df = processed_df.filter(col('error_message').isNull())\n",
    "    failed_df = processed_df.filter(col('error_message').isNotNull())\n",
    "    \n",
    "    success_count = success_df.count()\n",
    "    failed_count = failed_df.count()\n",
    "    \n",
    "    print(f\"   ‚úÖ Successful records: {success_count:,}\")\n",
    "    print(f\"   ‚ùå Failed records: {failed_count:,}\")\n",
    "    \n",
    "    # Write successful records\n",
    "    if success_count > 0:\n",
    "        success_df.drop('error_message').write.format('delta').mode('append').save(output_path)\n",
    "        print(f\"   üíæ Saved successful records to: {output_path}\")\n",
    "    \n",
    "    # Write failed records to DLQ\n",
    "    if failed_count > 0:\n",
    "        failed_df.withColumn('dlq_timestamp', current_timestamp()).write \\\n",
    "            .format('delta').mode('append').save(dlq_path)\n",
    "        print(f\"   üìÆ Sent failed records to DLQ: {dlq_path}\")\n",
    "    \n",
    "    return success_count, failed_count\n",
    "\n",
    "# Demo\n",
    "test_data_with_errors = spark.createDataFrame([\n",
    "    (1, 100, '2025-01-01', 'US'),\n",
    "    (2, -50, '2025-01-01', 'UK'),  # This will fail\n",
    "    (3, 200, '2025-01-02', 'US'),\n",
    "    (4, -100, '2025-01-02', 'JP'), # This will fail\n",
    "    (5, 300, '2025-01-03', 'US'),\n",
    "], ['id', 'amount', 'date', 'region'])\n",
    "\n",
    "success, failed = process_with_dlq(\n",
    "    test_data_with_errors,\n",
    "    '/tmp/processed_output',\n",
    "    '/tmp/dead_letter_queue'\n",
    ")\n",
    "\n",
    "# Show DLQ contents\n",
    "print(\"\\nDead Letter Queue contents:\")\n",
    "spark.read.format('delta').load('/tmp/dead_letter_queue').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Production Job Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-grade job template with all patterns combined\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class ProductionETLJob:\n",
    "    \"\"\"\n",
    "    Complete production ETL job with:\n",
    "    - Incremental processing\n",
    "    - Error handling & retries\n",
    "    - Dead letter queue\n",
    "    - Monitoring & alerting\n",
    "    - Data quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, job_name: str, config: Dict):\n",
    "        self.job_name = job_name\n",
    "        self.config = config\n",
    "        self.metrics = {\n",
    "            'job_name': job_name,\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'status': 'initialized',\n",
    "            'records_read': 0,\n",
    "            'records_processed': 0,\n",
    "            'records_failed': 0,\n",
    "            'errors': [],\n",
    "            'warnings': []\n",
    "        }\n",
    "    \n",
    "    def run_data_quality_checks(self, df):\n",
    "        \"\"\"Run data quality validations\"\"\"\n",
    "        print(\"\\nüîç Running data quality checks...\")\n",
    "        \n",
    "        checks = []\n",
    "        \n",
    "        # Check 1: No null keys\n",
    "        null_count = df.filter(col('id').isNull()).count()\n",
    "        checks.append(('null_ids', null_count == 0, f\"Found {null_count} null IDs\"))\n",
    "        \n",
    "        # Check 2: Valid amounts\n",
    "        invalid_amounts = df.filter((col('amount').isNull()) | (col('amount') < 0)).count()\n",
    "        checks.append(('valid_amounts', invalid_amounts == 0, f\"Found {invalid_amounts} invalid amounts\"))\n",
    "        \n",
    "        # Check 3: Record count threshold\n",
    "        count = df.count()\n",
    "        min_expected = self.config.get('min_records', 0)\n",
    "        checks.append(('record_count', count >= min_expected, \n",
    "                      f\"Record count {count} {'>=' if count >= min_expected else '<'} minimum {min_expected}\"))\n",
    "        \n",
    "        # Report\n",
    "        all_passed = True\n",
    "        for check_name, passed, message in checks:\n",
    "            status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "            print(f\"   {status} {check_name}: {message}\")\n",
    "            \n",
    "            if not passed:\n",
    "                all_passed = False\n",
    "                self.metrics['warnings'].append(f\"Data quality check failed: {check_name} - {message}\")\n",
    "        \n",
    "        return all_passed\n",
    "    \n",
    "    def process_data(self, input_df):\n",
    "        \"\"\"Main processing logic\"\"\"\n",
    "        # Your transformation logic here\n",
    "        result = input_df.groupBy('region', 'product').agg(\n",
    "            sum('amount').alias('total_sales'),\n",
    "            count('*').alias('transaction_count'),\n",
    "            avg('amount').alias('avg_sale')\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main job execution\"\"\"\n",
    "        self.metrics['start_time'] = datetime.now().isoformat()\n",
    "        self.metrics['status'] = 'running'\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üöÄ Starting job: {self.job_name}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Step 1: Read data\n",
    "            print(\"\\nüì• Step 1: Reading source data...\")\n",
    "            source_df = spark.read.format('delta').load(self.config['source_path'])\n",
    "            self.metrics['records_read'] = source_df.count()\n",
    "            print(f\"   Read {self.metrics['records_read']:,} records\")\n",
    "            \n",
    "            # Step 2: Data quality checks\n",
    "            quality_passed = self.run_data_quality_checks(source_df)\n",
    "            if not quality_passed and self.config.get('fail_on_quality_issues', False):\n",
    "                raise Exception(\"Data quality checks failed\")\n",
    "            \n",
    "            # Step 3: Process\n",
    "            print(\"\\n‚öôÔ∏è  Step 3: Processing data...\")\n",
    "            result_df = self.process_data(source_df)\n",
    "            self.metrics['records_processed'] = result_df.count()\n",
    "            print(f\"   Processed {self.metrics['records_processed']:,} records\")\n",
    "            \n",
    "            # Step 4: Write output\n",
    "            print(\"\\nüíæ Step 4: Writing output...\")\n",
    "            result_df.write \\\n",
    "                .format('delta') \\\n",
    "                .mode('overwrite') \\\n",
    "                .option('overwriteSchema', 'true') \\\n",
    "                .save(self.config['output_path'])\n",
    "            print(f\"   Output written to: {self.config['output_path']}\")\n",
    "            \n",
    "            # Success!\n",
    "            self.metrics['status'] = 'success'\n",
    "            print(\"\\n‚úÖ Job completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['status'] = 'failed'\n",
    "            self.metrics['errors'].append(str(e))\n",
    "            print(f\"\\n‚ùå Job failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            self.metrics['end_time'] = datetime.now().isoformat()\n",
    "            \n",
    "            # Log metrics\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"üìä JOB METRICS:\")\n",
    "            print(\"=\"*80)\n",
    "            print(json.dumps(self.metrics, indent=2))\n",
    "            \n",
    "            # In production: send to monitoring system\n",
    "            # self.send_metrics_to_cloudwatch(self.metrics)\n",
    "            # self.send_alerts_if_needed(self.metrics)\n",
    "        \n",
    "        return self.metrics\n",
    "\n",
    "# Example usage\n",
    "job_config = {\n",
    "    'source_path': delta_path,\n",
    "    'output_path': '/tmp/production_output',\n",
    "    'min_records': 5,\n",
    "    'fail_on_quality_issues': False\n",
    "}\n",
    "\n",
    "job = ProductionETLJob('customer_analytics_v2', job_config)\n",
    "metrics = job.run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Production job template complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Production Job Checklist\n",
    "\n",
    "### üéØ Essential Components:\n",
    "\n",
    "**1. Error Handling:**\n",
    "- [ ] Try-catch blocks around all operations\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Circuit breaker for external dependencies\n",
    "- [ ] Dead letter queue for failed records\n",
    "\n",
    "**2. Monitoring:**\n",
    "- [ ] Capture start/end times\n",
    "- [ ] Track record counts at each stage\n",
    "- [ ] Log all errors and warnings\n",
    "- [ ] Send metrics to monitoring system\n",
    "\n",
    "**3. Data Quality:**\n",
    "- [ ] Validate input data\n",
    "- [ ] Check for nulls, duplicates, outliers\n",
    "- [ ] Verify record count thresholds\n",
    "- [ ] Validate business logic constraints\n",
    "\n",
    "**4. Performance:**\n",
    "- [ ] Review execution plans\n",
    "- [ ] Optimize joins and aggregations\n",
    "- [ ] Appropriate partitioning\n",
    "- [ ] Cache only when needed\n",
    "\n",
    "**5. Reliability:**\n",
    "- [ ] Incremental processing with checkpoints\n",
    "- [ ] Idempotent operations\n",
    "- [ ] Transaction safety (Delta Lake)\n",
    "- [ ] Proper cleanup on failure\n",
    "\n",
    "**6. Alerting:**\n",
    "- [ ] Email/Slack on failure\n",
    "- [ ] PagerDuty for critical failures\n",
    "- [ ] Dashboard for job metrics\n",
    "- [ ] SLA monitoring\n",
    "\n",
    "### üìÖ Scheduling Best Practices:\n",
    "\n",
    "**Databricks Jobs:**\n",
    "```python\n",
    "# Configure via UI or Jobs API\n",
    "{\n",
    "  \"name\": \"customer_analytics_daily\",\n",
    "  \"schedule\": {\n",
    "    \"quartz_cron_expression\": \"0 0 2 * * ?\",  # 2 AM daily\n",
    "    \"timezone_id\": \"UTC\",\n",
    "    \"pause_status\": \"UNPAUSED\"\n",
    "  },\n",
    "  \"max_concurrent_runs\": 1,\n",
    "  \"timeout_seconds\": 3600,\n",
    "  \"max_retries\": 2,\n",
    "  \"retry_on_timeout\": true,\n",
    "  \"email_notifications\": {\n",
    "    \"on_failure\": [\"data-team@company.com\"],\n",
    "    \"on_success\": [],\n",
    "    \"no_alert_for_skipped_runs\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### üîó Useful Resources:\n",
    "- Databricks Jobs API: https://docs.databricks.com/dev-tools/api/latest/jobs.html\n",
    "- Delta Lake Best Practices: https://docs.delta.io/latest/best-practices.html\n",
    "- Spark Monitoring Guide: https://spark.apache.org/docs/latest/monitoring.html\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Workshop Complete!\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ Understanding of common performance pitfalls\n",
    "- ‚úÖ Production-ready job templates\n",
    "- ‚úÖ Error handling patterns\n",
    "- ‚úÖ Monitoring and alerting strategies\n",
    "- ‚úÖ Real-world scheduling examples\n",
    "\n",
    "**Next Steps:**\n",
    "1. Apply these patterns to your jobs\n",
    "2. Set up monitoring dashboards\n",
    "3. Implement incremental processing\n",
    "4. Configure alerts and retries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
