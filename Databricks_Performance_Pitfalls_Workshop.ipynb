{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Performance Pitfalls Workshop\n",
    "## Common Mistakes and How to Fix Them (70 mins)\n",
    "\n",
    "**Workshop Objectives:**\n",
    "- Identify and fix common Spark performance issues\n",
    "- Learn to read and interpret Spark UI and execution plans\n",
    "- Understand job scheduling and alerting strategies\n",
    "- Apply best practices for production workloads\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python/PySpark knowledge\n",
    "- Access to Databricks workspace\n",
    "\n",
    "**Datasets Used:**\n",
    "- Databricks sample datasets (diamonds, nyctaxi)\n",
    "- Synthetic data for specific examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create Sample Data\n",
    "\n",
    "Let's start by creating some sample data we'll use throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# In Databricks, spark session is already available\n",
    "# Set some initial configs\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Setup workspace path (works with restricted DBFS access)\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0].split('@')[0].replace('.', '_')\n",
    "base_path = f\"dbfs:/user/{username}/workshop_data\"\n",
    "\n",
    "# Alternative if above doesn't work: Use FileStore which is often accessible\n",
    "# base_path = f\"/dbfs/FileStore/workshop_data/{username}\"\n",
    "\n",
    "print(f\"\\nüìÅ Workshop data will be stored at: {base_path}\")\n",
    "print(\"   (This path works with restricted DBFS root access)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large customer transactions dataset\n",
    "# This will be used for demonstrating various performance issues\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Generate 1 million transaction records\n",
    "num_records = 1000000\n",
    "\n",
    "# Create skewed data (some customers have way more transactions)\n",
    "def generate_customer_id():\n",
    "    rand = random.random()\n",
    "    if rand < 0.3:  # 30% of transactions belong to just 10 customers (SKEW!)\n",
    "        return random.randint(1, 10)\n",
    "    else:\n",
    "        return random.randint(11, 10000)\n",
    "\n",
    "transactions_data = [\n",
    "    (\n",
    "        i,\n",
    "        generate_customer_id(),\n",
    "        round(random.uniform(10, 5000), 2),\n",
    "        random.choice(['Electronics', 'Clothing', 'Food', 'Books', 'Home']),\n",
    "        (datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        random.choice(['US/Pacific', 'US/Eastern', 'Europe/London', 'Asia/Tokyo'])\n",
    "    )\n",
    "    for i in range(num_records)\n",
    "]\n",
    "\n",
    "transactions_df = spark.createDataFrame(\n",
    "    transactions_data,\n",
    "    ['transaction_id', 'customer_id', 'amount', 'category', 'timestamp_str', 'timezone']\n",
    ")\n",
    "\n",
    "# Cache for reuse\n",
    "transactions_df.cache()\n",
    "print(f\"Created {transactions_df.count():,} transaction records\")\n",
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer dimension table\n",
    "customers_data = [\n",
    "    (i, f\"Customer_{i}\", random.choice(['Gold', 'Silver', 'Bronze', 'Platinum']),\n",
    "     random.choice(['USA', 'UK', 'Japan', 'Germany', 'France']))\n",
    "    for i in range(1, 10001)\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(\n",
    "    customers_data,\n",
    "    ['customer_id', 'customer_name', 'tier', 'country']\n",
    ")\n",
    "\n",
    "customers_df.cache()\n",
    "print(f\"Created {customers_df.count():,} customer records\")\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #1: Shuffle Explosion (Wide Transformations)\n",
    "\n",
    "**What is it?**  \n",
    "Shuffle operations move data across executors and are expensive. They occur during operations like `groupBy`, `join`, `repartition`, and `distinct`.\n",
    "\n",
    "**Problem:** Unnecessary or excessive shuffles can kill performance.\n",
    "\n",
    "**Time: 7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Multiple unnecessary shuffles\n",
    "print(\"BAD APPROACH: Multiple shuffles\")\n",
    "\n",
    "bad_result = transactions_df \\\n",
    "    .repartition(200) \\\n",
    "    .groupBy('customer_id').agg(sum('amount').alias('total_1')) \\\n",
    "    .repartition(100) \\\n",
    "    .groupBy('customer_id').agg(sum('total_1').alias('total_2')) \\\n",
    "    .repartition(50)\n",
    "\n",
    "# Look at the execution plan - notice all the \"Exchange\" operations (shuffles)\n",
    "bad_result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Minimize shuffles\n",
    "print(\"GOOD APPROACH: Optimized shuffles\")\n",
    "\n",
    "good_result = transactions_df \\\n",
    "    .groupBy('customer_id').agg(\n",
    "        sum('amount').alias('total_amount'),\n",
    "        count('transaction_id').alias('transaction_count'),\n",
    "        avg('amount').alias('avg_amount')\n",
    "    )\n",
    "\n",
    "# Much cleaner execution plan with only necessary shuffles\n",
    "good_result.explain()\n",
    "good_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- Use `explain()` to see execution plans and identify shuffles (look for \"Exchange\")\n",
    "- Combine aggregations into single operations\n",
    "- Avoid unnecessary `repartition()` calls\n",
    "- Let Adaptive Query Execution handle partition sizing when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #2: Skewed Keys\n",
    "\n",
    "**What is it?**  \n",
    "When data is unevenly distributed across keys, some partitions get overwhelmed while others sit idle.\n",
    "\n",
    "**Problem:** One executor does all the work, causing stragglers and potential OOM errors.\n",
    "\n",
    "**Time: 7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify the skew\n",
    "print(\"Checking for skewed data distribution...\")\n",
    "\n",
    "skew_check = transactions_df \\\n",
    "    .groupBy('customer_id') \\\n",
    "    .agg(count('*').alias('transaction_count')) \\\n",
    "    .orderBy(col('transaction_count').desc())\n",
    "\n",
    "skew_check.show(20)\n",
    "\n",
    "# Notice: Top customers have WAY more transactions than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Direct join with skewed data\n",
    "print(\"BAD APPROACH: Join with skewed keys\")\n",
    "\n",
    "# This will have terrible performance due to skew\n",
    "bad_join = transactions_df \\\n",
    "    .join(customers_df, 'customer_id') \\\n",
    "    .groupBy('customer_name', 'tier') \\\n",
    "    .agg(sum('amount').alias('total_spent'))\n",
    "\n",
    "# Check the plan - you'll see uneven partition sizes in Spark UI\n",
    "bad_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Use salting technique for skewed joins\n",
    "print(\"GOOD APPROACH: Salted join to handle skew\")\n",
    "\n",
    "# Enable skew join optimization (Databricks/Spark 3.0+)\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
    "\n",
    "# Adaptive Query Execution will automatically handle skew!\n",
    "good_join = transactions_df \\\n",
    "    .join(customers_df, 'customer_id') \\\n",
    "    .groupBy('customer_name', 'tier') \\\n",
    "    .agg(sum('amount').alias('total_spent'))\n",
    "\n",
    "good_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manual salting for extreme cases\n",
    "print(\"ALTERNATIVE: Manual salting technique\")\n",
    "\n",
    "# Add salt to skewed keys\n",
    "salt_size = 10\n",
    "\n",
    "transactions_salted = transactions_df \\\n",
    "    .withColumn('salt', (rand() * salt_size).cast('int')) \\\n",
    "    .withColumn('salted_key', concat(col('customer_id').cast('string'), lit('_'), col('salt')))\n",
    "\n",
    "# Explode the small dimension table\n",
    "from pyspark.sql.functions import explode, array, lit as sql_lit\n",
    "\n",
    "customers_exploded = customers_df \\\n",
    "    .withColumn('salt', explode(array([sql_lit(i) for i in range(salt_size)]))) \\\n",
    "    .withColumn('salted_key', concat(col('customer_id').cast('string'), lit('_'), col('salt')))\n",
    "\n",
    "# Join on salted keys\n",
    "salted_join = transactions_salted \\\n",
    "    .join(customers_exploded, 'salted_key') \\\n",
    "    .groupBy('customer_name', 'tier') \\\n",
    "    .agg(sum('amount').alias('total_spent'))\n",
    "\n",
    "salted_join.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- Always check data distribution before joins/aggregations\n",
    "- Enable Adaptive Query Execution and skew join optimization\n",
    "- For extreme skew, use salting techniques\n",
    "- Monitor Spark UI to identify straggler tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #3: Join Strategy Not Optimized\n",
    "\n",
    "**What is it?**  \n",
    "Spark offers different join strategies (Broadcast, Sort-Merge, Shuffle Hash). Wrong choice = slow performance.\n",
    "\n",
    "**Problem:** Large tables get broadcast, small tables don't, causing unnecessary shuffles.\n",
    "\n",
    "**Time: 6 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current broadcast threshold\n",
    "print(f\"Current broadcast threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')}\")\n",
    "\n",
    "# ‚ùå BAD: Not using broadcast for small tables\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable auto broadcast\n",
    "\n",
    "bad_join_strategy = transactions_df \\\n",
    "    .join(customers_df, 'customer_id') \\\n",
    "    .select('transaction_id', 'customer_name', 'amount')\n",
    "\n",
    "print(\"\\nBAD: Sort-Merge Join (when broadcast would be better)\")\n",
    "bad_join_strategy.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Use broadcast hint for small tables\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB (default)\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "good_join_strategy = transactions_df \\\n",
    "    .join(broadcast(customers_df), 'customer_id') \\\n",
    "    .select('transaction_id', 'customer_name', 'amount')\n",
    "\n",
    "print(\"\\nGOOD: Broadcast Hash Join\")\n",
    "good_join_strategy.explain()\n",
    "good_join_strategy.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- Broadcast small tables (< 10MB) to avoid shuffles\n",
    "- Use `broadcast()` hint to force broadcast joins\n",
    "- Check `explain()` output to verify join strategy\n",
    "- **Join types in order of preference:**\n",
    "  1. Broadcast Hash Join (fastest, no shuffle)\n",
    "  2. Shuffle Hash Join\n",
    "  3. Sort-Merge Join (default for large tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #4: Python UDF Slowness\n",
    "\n",
    "**What is it?**  \n",
    "Python UDFs serialize data between JVM and Python, losing Spark's optimizations.\n",
    "\n",
    "**Problem:** 10-100x slower than native Spark operations!\n",
    "\n",
    "**Time: 7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Python UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "\n",
    "@udf(StringType())\n",
    "def categorize_amount_udf(amount):\n",
    "    \"\"\"Slow Python UDF\"\"\"\n",
    "    if amount < 100:\n",
    "        return 'Small'\n",
    "    elif amount < 1000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "print(\"BAD: Using Python UDF\")\n",
    "start = time.time()\n",
    "\n",
    "bad_udf_result = transactions_df \\\n",
    "    .withColumn('amount_category', categorize_amount_udf(col('amount'))) \\\n",
    "    .select('transaction_id', 'amount', 'amount_category')\n",
    "\n",
    "bad_udf_result.write.mode('overwrite').format('noop').save()  # Trigger execution\n",
    "bad_time = time.time() - start\n",
    "\n",
    "print(f\"Python UDF execution time: {bad_time:.2f} seconds\")\n",
    "bad_udf_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Native Spark SQL functions\n",
    "print(\"GOOD: Using native Spark functions\")\n",
    "start = time.time()\n",
    "\n",
    "good_native_result = transactions_df \\\n",
    "    .withColumn('amount_category', \n",
    "                when(col('amount') < 100, 'Small')\n",
    "                .when(col('amount') < 1000, 'Medium')\n",
    "                .otherwise('Large')) \\\n",
    "    .select('transaction_id', 'amount', 'amount_category')\n",
    "\n",
    "good_native_result.write.mode('overwrite').format('noop').save()  # Trigger execution\n",
    "good_time = time.time() - start\n",
    "\n",
    "print(f\"Native Spark execution time: {good_time:.2f} seconds\")\n",
    "print(f\"Speedup: {bad_time/good_time:.1f}x faster!\")\n",
    "good_native_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ BETTER: Pandas UDF (if you must use Python)\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def categorize_amount_pandas(amounts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Faster Pandas UDF - vectorized processing\"\"\"\n",
    "    return pd.cut(amounts, \n",
    "                  bins=[-float('inf'), 100, 1000, float('inf')],\n",
    "                  labels=['Small', 'Medium', 'Large'])\n",
    "\n",
    "print(\"BETTER: Using Pandas UDF\")\n",
    "start = time.time()\n",
    "\n",
    "pandas_udf_result = transactions_df \\\n",
    "    .withColumn('amount_category', categorize_amount_pandas(col('amount'))) \\\n",
    "    .select('transaction_id', 'amount', 'amount_category')\n",
    "\n",
    "pandas_udf_result.write.mode('overwrite').format('noop').save()\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "print(f\"Pandas UDF execution time: {pandas_time:.2f} seconds\")\n",
    "print(f\"Pandas UDF is {bad_time/pandas_time:.1f}x faster than Python UDF\")\n",
    "pandas_udf_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- **Avoid Python UDFs whenever possible!**\n",
    "- Use native Spark SQL functions (when, case, etc.)\n",
    "- If you need Python, use Pandas UDFs (vectorized)\n",
    "- Performance hierarchy: Native Spark > Pandas UDF > Python UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #5: Ineffective Caching/Persistence\n",
    "\n",
    "**What is it?**  \n",
    "Caching stores DataFrames in memory for reuse, but misuse wastes memory or doesn't help.\n",
    "\n",
    "**Problem:** Cache too much = OOM. Cache wrong things = no benefit.\n",
    "\n",
    "**Time: 6 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Caching everything or caching before filtering\n",
    "print(\"BAD: Caching before filtering (wastes memory)\")\n",
    "\n",
    "# Don't do this - caches the full dataset!\n",
    "bad_cache = transactions_df.cache()\n",
    "filtered_data = bad_cache.filter(col('amount') > 1000)  # Only need this subset!\n",
    "\n",
    "print(f\"Cached full dataset: {bad_cache.storageLevel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up bad cache\n",
    "bad_cache.unpersist()\n",
    "\n",
    "# ‚úÖ GOOD: Cache after filtering, only when reusing\n",
    "print(\"GOOD: Cache after filtering, when data is reused multiple times\")\n",
    "\n",
    "# Filter first, then cache\n",
    "high_value_transactions = transactions_df \\\n",
    "    .filter(col('amount') > 1000) \\\n",
    "    .cache()\n",
    "\n",
    "# Trigger caching\n",
    "count = high_value_transactions.count()\n",
    "print(f\"Cached {count:,} high-value transactions\")\n",
    "\n",
    "# Now reuse the cached data multiple times (this is when cache helps!)\n",
    "result1 = high_value_transactions.groupBy('category').agg(sum('amount'))\n",
    "result2 = high_value_transactions.groupBy('customer_id').agg(count('*'))\n",
    "result3 = high_value_transactions.agg(avg('amount'), max('amount'))\n",
    "\n",
    "result1.show()\n",
    "result2.show(5)\n",
    "result3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different storage levels for different use cases\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# MEMORY_ONLY (default) - fastest but can cause OOM\n",
    "memory_only = transactions_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# MEMORY_AND_DISK - spills to disk if memory full (safer)\n",
    "memory_and_disk = transactions_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# MEMORY_AND_DISK_SER - serialized (saves memory, slower access)\n",
    "serialized = transactions_df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "print(\"\\n‚úÖ Best Practices:\")\n",
    "print(\"- Cache AFTER filtering/transformation\")\n",
    "print(\"- Only cache if you'll reuse the data 2+ times\")\n",
    "print(\"- Use MEMORY_AND_DISK for production (prevents OOM)\")\n",
    "print(\"- Always unpersist() when done to free memory\")\n",
    "\n",
    "# Clean up\n",
    "high_value_transactions.unpersist()\n",
    "memory_only.unpersist()\n",
    "memory_and_disk.unpersist()\n",
    "serialized.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- Only cache data that will be reused multiple times\n",
    "- Cache AFTER filtering/transformations to reduce memory usage\n",
    "- Use `MEMORY_AND_DISK` for production workloads\n",
    "- Always `unpersist()` when done to free up memory\n",
    "- Check Spark UI ‚Üí Storage tab to monitor cached data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #6: Timezone / Timestamp Parsing Issues\n",
    "\n",
    "**What is it?**  \n",
    "Timestamp parsing and timezone handling can cause incorrect results and performance issues.\n",
    "\n",
    "**Problem:** Wrong timestamps, daylight saving bugs, slow parsing.\n",
    "\n",
    "**Time: 6 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Parsing timestamps without timezone awareness\n",
    "print(\"BAD: Naive timestamp parsing\")\n",
    "\n",
    "# This can give wrong results depending on cluster timezone\n",
    "bad_timestamp = transactions_df \\\n",
    "    .withColumn('parsed_time', to_timestamp(col('timestamp_str'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "bad_timestamp.select('timestamp_str', 'timezone', 'parsed_time').show(5, truncate=False)\n",
    "\n",
    "# Problem: All timestamps are treated as if they're in the same timezone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Timezone-aware timestamp handling\n",
    "print(\"GOOD: Timezone-aware parsing\")\n",
    "\n",
    "# Set session timezone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "good_timestamp = transactions_df \\\n",
    "    .withColumn('parsed_time', to_timestamp(col('timestamp_str'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
    "    .withColumn('utc_time', \n",
    "                to_utc_timestamp(col('parsed_time'), col('timezone')))\n",
    "\n",
    "good_timestamp.select('timestamp_str', 'timezone', 'parsed_time', 'utc_time').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Using Python UDF for date parsing (slow!)\n",
    "from datetime import datetime as dt\n",
    "\n",
    "@udf(StringType())\n",
    "def extract_hour_bad(timestamp_str):\n",
    "    return dt.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S').strftime('%H')\n",
    "\n",
    "print(\"BAD: Python UDF for date operations\")\n",
    "bad_hour = transactions_df.withColumn('hour', extract_hour_bad(col('timestamp_str')))\n",
    "bad_hour.select('timestamp_str', 'hour').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Native Spark date/time functions\n",
    "print(\"GOOD: Native Spark date functions\")\n",
    "\n",
    "good_date_ops = transactions_df \\\n",
    "    .withColumn('parsed_time', to_timestamp(col('timestamp_str'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
    "    .withColumn('hour', hour(col('parsed_time'))) \\\n",
    "    .withColumn('day_of_week', dayofweek(col('parsed_time'))) \\\n",
    "    .withColumn('quarter', quarter(col('parsed_time'))) \\\n",
    "    .withColumn('date', to_date(col('parsed_time')))\n",
    "\n",
    "good_date_ops.select('timestamp_str', 'hour', 'day_of_week', 'quarter', 'date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- Always set `spark.sql.session.timeZone` explicitly (prefer UTC)\n",
    "- Use `to_utc_timestamp()` and `from_utc_timestamp()` for timezone conversions\n",
    "- Use native Spark date functions: `hour()`, `dayofweek()`, `date_format()`, etc.\n",
    "- Store timestamps in UTC in your data lake\n",
    "- Be aware of daylight saving time issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #7: Bad Partitioning Strategy\n",
    "\n",
    "**What is it?**  \n",
    "Data partitioning affects how data is organized on disk and in memory.\n",
    "\n",
    "**Problem:** Too many partitions = overhead. Too few = no parallelism. Wrong key = slow queries.\n",
    "\n",
    "**Time: 7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current partitioning\n",
    "print(f\"Current DataFrame partitions: {transactions_df.rdd.getNumPartitions()}\")\n",
    "print(f\"Current DataFrame records: {transactions_df.count():,}\")\n",
    "print(f\"Records per partition: {transactions_df.count() / transactions_df.rdd.getNumPartitions():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Too many small partitions\n",
    "print(\"BAD: Too many partitions (overhead!)\")\n",
    "\n",
    "bad_partitioning = transactions_df.repartition(5000)  # Way too many!\n",
    "print(f\"Partitions: {bad_partitioning.rdd.getNumPartitions()}\")\n",
    "print(f\"Records per partition: {transactions_df.count() / bad_partitioning.rdd.getNumPartitions():.0f}\")\n",
    "print(\"‚ö†Ô∏è Only 200 records per partition - massive overhead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: Too few large partitions\n",
    "print(\"BAD: Too few partitions (no parallelism!)\")\n",
    "\n",
    "bad_partitioning2 = transactions_df.coalesce(2)  # Only 2 partitions!\n",
    "print(f\"Partitions: {bad_partitioning2.rdd.getNumPartitions()}\")\n",
    "print(f\"Records per partition: {transactions_df.count() / bad_partitioning2.rdd.getNumPartitions():,.0f}\")\n",
    "print(\"‚ö†Ô∏è 500k records per partition - can't utilize cluster parallelism!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD: Appropriate partition sizing\n",
    "print(\"GOOD: Right-sized partitions\")\n",
    "\n",
    "# Rule of thumb: 128MB - 1GB per partition\n",
    "# For 1M records, 100-200 partitions is reasonable\n",
    "\n",
    "good_partitioning = transactions_df.repartition(100)\n",
    "print(f\"Partitions: {good_partitioning.rdd.getNumPartitions()}\")\n",
    "print(f\"Records per partition: {transactions_df.count() / good_partitioning.rdd.getNumPartitions():,.0f}\")\n",
    "print(\"‚úÖ ~10k records per partition - good balance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ BETTER: Partition by logical keys (for disk storage)\n",
    "print(\"BETTER: Partitioning by logical columns for queries\")\n",
    "\n",
    "# Add date column for partitioning\n",
    "partitioned_data = transactions_df \\\n",
    "    .withColumn('date', to_date(to_timestamp(col('timestamp_str'), 'yyyy-MM-dd HH:mm:ss'))) \\\n",
    "    .withColumn('year', year(col('date'))) \\\n",
    "    .withColumn('month', month(col('date')))\n",
    "\n",
    "# Write partitioned by year/month (common for time-series data)\n",
    "output_path = f\"{base_path}/transactions_partitioned\"\n",
    "\n",
    "partitioned_data.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('year', 'month') \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"‚úÖ Data written partitioned by year/month\")\n",
    "print(\"Benefits: Query pruning when filtering by date!\")\n",
    "\n",
    "# Read back and show partition pruning\n",
    "partitioned_read = spark.read.parquet(output_path)\n",
    "filtered_query = partitioned_read.filter((col('year') == 2025) & (col('month') == 11))\n",
    "print(\"\\nExecution plan (notice partition pruning):\")\n",
    "filtered_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- **Target 128MB - 1GB per partition**\n",
    "- Use `repartition()` to increase partitions (full shuffle)\n",
    "- Use `coalesce()` to decrease partitions (no shuffle)\n",
    "- Partition by query patterns (e.g., date for time-series)\n",
    "- Avoid high-cardinality partition keys\n",
    "- **Formula:** `partitions = data_size_MB / 128`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pitfall #8: Not Reading Spark UI and Query Plans\n",
    "\n",
    "**What is it?**  \n",
    "Spark UI and execution plans provide crucial insights into what Spark is actually doing.\n",
    "\n",
    "**Problem:** Flying blind leads to mystery performance issues.\n",
    "\n",
    "**Time: 7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex query to demonstrate plan reading\n",
    "complex_query = transactions_df \\\n",
    "    .filter(col('amount') > 100) \\\n",
    "    .join(broadcast(customers_df), 'customer_id') \\\n",
    "    .groupBy('tier', 'category') \\\n",
    "    .agg(\n",
    "        sum('amount').alias('total_amount'),\n",
    "        count('*').alias('transaction_count'),\n",
    "        avg('amount').alias('avg_amount')\n",
    "    ) \\\n",
    "    .orderBy(col('total_amount').desc())\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SIMPLE EXPLAIN - High level overview\")\n",
    "print(\"=\" * 80)\n",
    "complex_query.explain()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTENDED EXPLAIN - Shows parsed, analyzed, optimized, and physical plans\")\n",
    "print(\"=\" * 80)\n",
    "complex_query.explain(extended=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FORMATTED EXPLAIN - Easier to read tree structure\")\n",
    "print(\"=\" * 80)\n",
    "complex_query.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key things to look for in execution plans\n",
    "print(\"\"\"\n",
    "üìä HOW TO READ EXECUTION PLANS:\n",
    "\n",
    "1. READ BOTTOM-TO-TOP (data flows upward)\n",
    "\n",
    "2. LOOK FOR THESE OPERATIONS:\n",
    "   ‚úÖ BroadcastHashJoin - Good! Small table broadcast\n",
    "   ‚ö†Ô∏è  SortMergeJoin - OK for large-large joins\n",
    "   ‚ö†Ô∏è  Exchange - Shuffle operation (expensive)\n",
    "   ‚ö†Ô∏è  Sort - Full sort (expensive)\n",
    "   ‚ùå CartesianProduct - BAD! Cross join\n",
    "   \n",
    "3. PARTITION INFORMATION:\n",
    "   - Look for partition counts in Exchange operations\n",
    "   - Check for partition skew warnings\n",
    "   \n",
    "4. FILTER PUSHDOWN:\n",
    "   - Filters should appear early (bottom of plan)\n",
    "   - PushedFilters means optimization worked!\n",
    "   \n",
    "5. ADAPTIVE QUERY EXECUTION:\n",
    "   - AdaptiveSparkPlan shows AQE is active\n",
    "   - Look for dynamic optimizations\n",
    "\"\"\")\n",
    "\n",
    "# Execute to show in Spark UI\n",
    "result = complex_query.collect()\n",
    "print(\"\\n‚úÖ Query executed - Now check Spark UI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how to identify problems in plans\n",
    "print(\"\"\"\n",
    "üîç SPARK UI CHECKLIST:\n",
    "\n",
    "üìç Jobs Tab:\n",
    "   - Which stages took longest?\n",
    "   - Any failed tasks?\n",
    "   - Task distribution across executors\n",
    "\n",
    "üìç Stages Tab:\n",
    "   - Task timing (look for stragglers)\n",
    "   - Shuffle read/write volumes\n",
    "   - GC time (high = memory issues)\n",
    "   - Spill to disk (bad - not enough memory)\n",
    "\n",
    "üìç Storage Tab:\n",
    "   - What's cached?\n",
    "   - How much memory used?\n",
    "   - Which partitions are cached?\n",
    "\n",
    "üìç SQL Tab:\n",
    "   - Query execution timeline\n",
    "   - Physical plan visualization\n",
    "   - Click on SQL query to see DAG\n",
    "\n",
    "üö® RED FLAGS:\n",
    "   - Tasks with 10x+ longer duration than median (skew!)\n",
    "   - Large shuffle writes (> 1GB)\n",
    "   - High GC time (> 10% of task time)\n",
    "   - Spill to disk (means insufficient memory)\n",
    "   - Many small tasks (< 1s each = too many partitions)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Takeaways:**\n",
    "- **Always run `explain()` before executing expensive queries**\n",
    "- Learn to read execution plans (bottom-to-top)\n",
    "- Check Spark UI for every production job\n",
    "- Focus on: shuffles, joins, task skew, and GC time\n",
    "- Use `explain(mode='formatted')` for easier reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Real-World Example: Building a Production Pipeline\n",
    "\n",
    "**Time: 8 minutes**\n",
    "\n",
    "Let's put it all together and build a production-quality ETL pipeline that applies all best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready pipeline incorporating all best practices\n",
    "def customer_analytics_pipeline():\n",
    "    \"\"\"\n",
    "    A production-grade pipeline that calculates customer analytics\n",
    "    with proper error handling, optimization, and monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Customer Analytics Pipeline...\")\n",
    "    \n",
    "    # Step 1: Load and validate data\n",
    "    print(\"\\nüì• Step 1: Loading data...\")\n",
    "    transactions = transactions_df.filter(col('amount').isNotNull())  # Filter early!\n",
    "    customers = customers_df\n",
    "    \n",
    "    # Step 2: Parse timestamps correctly\n",
    "    print(\"\\n‚è∞ Step 2: Processing timestamps...\")\n",
    "    transactions_with_date = transactions \\\n",
    "        .withColumn('parsed_time', to_timestamp(col('timestamp_str'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
    "        .withColumn('date', to_date(col('parsed_time'))) \\\n",
    "        .withColumn('hour', hour(col('parsed_time'))) \\\n",
    "        .drop('timestamp_str')  # Drop unused columns to save memory\n",
    "    \n",
    "    # Step 3: Check for and handle data skew\n",
    "    print(\"\\n‚öñÔ∏è  Step 3: Checking for skewed keys...\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    \n",
    "    # Step 4: Optimized join with broadcast hint\n",
    "    print(\"\\nüîó Step 4: Joining with customers (broadcast)...\")\n",
    "    enriched = transactions_with_date \\\n",
    "        .join(broadcast(customers), 'customer_id')  # Broadcast small table\n",
    "    \n",
    "    # Step 5: Cache intermediate result (reused multiple times)\n",
    "    print(\"\\nüíæ Step 5: Caching enriched data...\")\n",
    "    enriched.persist(StorageLevel.MEMORY_AND_DISK)  # Safe caching\n",
    "    enriched.count()  # Materialize cache\n",
    "    \n",
    "    # Step 6: Calculate metrics using native Spark functions (no UDFs!)\n",
    "    print(\"\\nüìä Step 6: Calculating customer metrics...\")\n",
    "    customer_metrics = enriched.groupBy('customer_id', 'customer_name', 'tier', 'country').agg(\n",
    "        sum('amount').alias('total_spent'),\n",
    "        count('transaction_id').alias('transaction_count'),\n",
    "        avg('amount').alias('avg_transaction'),\n",
    "        max('amount').alias('max_transaction'),\n",
    "        countDistinct('category').alias('categories_purchased'),\n",
    "        min('date').alias('first_purchase'),\n",
    "        max('date').alias('last_purchase')\n",
    "    )\n",
    "    \n",
    "    # Step 7: Category analysis\n",
    "    print(\"\\nüìà Step 7: Analyzing by category and tier...\")\n",
    "    category_metrics = enriched.groupBy('tier', 'category').agg(\n",
    "        sum('amount').alias('total_revenue'),\n",
    "        count('*').alias('transaction_count'),\n",
    "        countDistinct('customer_id').alias('unique_customers')\n",
    "    )\n",
    "    \n",
    "    # Step 8: Write results with proper partitioning\n",
    "    print(\"\\nüíæ Step 8: Writing results...\")\n",
    "    \n",
    "    customer_metrics \\\n",
    "        .repartition(10) \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .format('parquet') \\\n",
    "        .option('compression', 'snappy') \\\n",
    "        .save(f'{base_path}/customer_metrics')\n",
    "    \n",
    "    category_metrics \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .partitionBy('tier') \\\n",
    "        .format('parquet') \\\n",
    "        .save(f'{base_path}/category_metrics')\n",
    "    \n",
    "    # Step 9: Clean up\n",
    "    print(\"\\nüßπ Step 9: Cleaning up...\")\n",
    "    enriched.unpersist()\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    return customer_metrics, category_metrics\n",
    "\n",
    "# Run the pipeline\n",
    "customer_results, category_results = customer_analytics_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUSTOMER METRICS SAMPLE:\")\n",
    "customer_results.orderBy(col('total_spent').desc()).show(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY METRICS SAMPLE:\")\n",
    "category_results.orderBy(col('total_revenue').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Job Scheduling & Monitoring in Production\n",
    "\n",
    "**Time: 10 minutes**\n",
    "\n",
    "Now let's see how to schedule and monitor this pipeline in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks Jobs Configuration\n",
    "\n",
    "To schedule this notebook as a Databricks Job:\n",
    "\n",
    "1. **Go to Workflows ‚Üí Jobs ‚Üí Create Job**\n",
    "\n",
    "2. **Configure Job:**\n",
    "   - **Name:** `customer_analytics_daily`\n",
    "   - **Task Type:** Notebook\n",
    "   - **Notebook Path:** Path to this notebook\n",
    "   - **Cluster:** Choose cluster or create new\n",
    "\n",
    "3. **Schedule:**\n",
    "   - **Trigger:** Scheduled\n",
    "   - **Cron:** `0 2 * * *` (runs at 2 AM daily)\n",
    "   - **Timezone:** UTC\n",
    "\n",
    "4. **Advanced Settings:**\n",
    "   - **Timeout:** 3600 seconds (1 hour)\n",
    "   - **Retries:** 2\n",
    "   - **Retry interval:** 300 seconds (5 minutes)\n",
    "   - **Max concurrent runs:** 1 (prevent overlaps)\n",
    "\n",
    "5. **Alerts:**\n",
    "   - Email on failure\n",
    "   - Slack/PagerDuty integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding monitoring and alerting to your pipeline\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def run_with_monitoring():\n",
    "    \"\"\"\n",
    "    Production pipeline with monitoring, error handling, and alerting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'job_name': 'customer_analytics_pipeline',\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'status': 'running',\n",
    "        'records_processed': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run pipeline\n",
    "        customer_results, category_results = customer_analytics_pipeline()\n",
    "        \n",
    "        # Collect metrics\n",
    "        metrics['records_processed'] = customer_results.count()\n",
    "        metrics['status'] = 'success'\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(\"\\nüîç Running data quality checks...\")\n",
    "        \n",
    "        # Check 1: No null customer IDs\n",
    "        null_customers = customer_results.filter(col('customer_id').isNull()).count()\n",
    "        if null_customers > 0:\n",
    "            metrics['errors'].append(f\"Found {null_customers} null customer IDs\")\n",
    "        \n",
    "        # Check 2: All amounts are positive\n",
    "        negative_amounts = customer_results.filter(col('total_spent') < 0).count()\n",
    "        if negative_amounts > 0:\n",
    "            metrics['errors'].append(f\"Found {negative_amounts} negative amounts\")\n",
    "        \n",
    "        # Check 3: Record count within expected range\n",
    "        expected_min_customers = 1000\n",
    "        actual_customers = customer_results.count()\n",
    "        if actual_customers < expected_min_customers:\n",
    "            metrics['errors'].append(\n",
    "                f\"Customer count {actual_customers} below threshold {expected_min_customers}\"\n",
    "            )\n",
    "        \n",
    "        metrics['data_quality_checks'] = {\n",
    "            'null_customers': null_customers,\n",
    "            'negative_amounts': negative_amounts,\n",
    "            'total_customers': actual_customers\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        metrics['status'] = 'failed'\n",
    "        metrics['errors'].append(str(e))\n",
    "        print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        metrics['end_time'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Log metrics (in production, send to monitoring system)\n",
    "        print(\"\\nüìä Pipeline Metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "        \n",
    "        # In production, you would:\n",
    "        # - Send metrics to CloudWatch/DataDog/etc.\n",
    "        # - Trigger alerts if errors exist\n",
    "        # - Update job status dashboard\n",
    "        \n",
    "        if metrics['errors']:\n",
    "            print(\"\\n‚ö†Ô∏è  ALERTS TRIGGERED:\")\n",
    "            for error in metrics['errors']:\n",
    "                print(f\"   - {error}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run with monitoring\n",
    "job_metrics = run_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Job Checklist\n",
    "\n",
    "‚úÖ **Before Deploying:**\n",
    "\n",
    "1. **Performance:**\n",
    "   - [ ] Reviewed execution plans with `explain()`\n",
    "   - [ ] Checked Spark UI for bottlenecks\n",
    "   - [ ] Optimized joins (broadcast where appropriate)\n",
    "   - [ ] Validated partition sizing\n",
    "   - [ ] Removed unnecessary caching\n",
    "   - [ ] Used native Spark functions (no Python UDFs)\n",
    "\n",
    "2. **Reliability:**\n",
    "   - [ ] Added error handling and retries\n",
    "   - [ ] Implemented data quality checks\n",
    "   - [ ] Set appropriate timeouts\n",
    "   - [ ] Configured alerts (email/Slack)\n",
    "   - [ ] Prevented concurrent runs\n",
    "   - [ ] Added logging and metrics\n",
    "\n",
    "3. **Monitoring:**\n",
    "   - [ ] Set up success/failure alerts\n",
    "   - [ ] Track job duration trends\n",
    "   - [ ] Monitor data volume changes\n",
    "   - [ ] Track data quality metrics\n",
    "   - [ ] Set up dashboard for key metrics\n",
    "\n",
    "4. **Cost Optimization:**\n",
    "   - [ ] Right-sized cluster for workload\n",
    "   - [ ] Using spot instances where appropriate\n",
    "   - [ ] Auto-scaling enabled\n",
    "   - [ ] Job timeouts to prevent runaway costs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Best Practices Recap\n",
    "\n",
    "**Time: 5 minutes**\n",
    "\n",
    "### Top 10 Performance Rules:\n",
    "\n",
    "1. **Always use `explain()` before executing expensive queries**\n",
    "2. **Minimize shuffles** - combine operations, avoid unnecessary repartitions\n",
    "3. **Broadcast small tables** (< 10MB) in joins\n",
    "4. **Never use Python UDFs** - use native Spark functions or Pandas UDFs\n",
    "5. **Enable Adaptive Query Execution** and skew join optimization\n",
    "6. **Cache intelligently** - only reused data, after filtering, with MEMORY_AND_DISK\n",
    "7. **Partition smartly** - target 128MB-1GB per partition, use logical keys\n",
    "8. **Handle timezones properly** - store in UTC, use native date functions\n",
    "9. **Monitor Spark UI** - check for skew, shuffles, GC time, spills\n",
    "10. **Implement proper job scheduling** - retries, alerts, quality checks\n",
    "\n",
    "### Production Checklist:\n",
    "- ‚úÖ Execution plan reviewed\n",
    "- ‚úÖ Spark UI analyzed\n",
    "- ‚úÖ Joins optimized\n",
    "- ‚úÖ Caching appropriate\n",
    "- ‚úÖ Error handling added\n",
    "- ‚úÖ Quality checks implemented\n",
    "- ‚úÖ Alerts configured\n",
    "- ‚úÖ Monitoring enabled\n",
    "- ‚úÖ Documentation complete\n",
    "\n",
    "### Resources:\n",
    "- Databricks Documentation: https://docs.databricks.com\n",
    "- Spark UI Guide: https://spark.apache.org/docs/latest/web-ui.html\n",
    "- Performance Tuning: https://spark.apache.org/docs/latest/tuning.html\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Workshop Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Try these techniques on your own data\n",
    "2. Schedule this notebook as a Databricks Job\n",
    "3. Set up monitoring and alerts\n",
    "4. Experiment with the other advanced topics (see companion notebook)\n",
    "\n",
    "**Questions?** Check out the companion notebooks for:\n",
    "- Delta Lake best practices\n",
    "- Advanced partitioning strategies\n",
    "- File format comparisons\n",
    "- Complex job orchestration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
